from transformers import LlavaForConditionalGeneration, LlavaProcessor
from PIL import Image
import torch
import os

class LlavaModel:
    def __init__(self, model_id="llava-hf/llava-1.5-7b-hf"):
        """LLaVA ëª¨ë¸ ì´ˆê¸°í™”"""
        print(f"ğŸ”„ ëª¨ë¸ ë¡œë”© ì¤‘: {model_id}")
        
        # ë””ë°”ì´ìŠ¤ í™•ì¸
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
        
        self.model = LlavaForConditionalGeneration.from_pretrained(
            model_id, 
            device_map="auto",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
        )
        self.processor = LlavaProcessor.from_pretrained(model_id)
        self.model_id = model_id
        print("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
    
    def generate_response(self, image_path, question, max_tokens=500):
        """ë‹¨ì¼ ì´ë¯¸ì§€ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
        try:
            # ì´ë¯¸ì§€ ì¡´ì¬ í™•ì¸
            if not os.path.exists(image_path):
                raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            img = Image.open(image_path).convert("RGB")
            
            # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ (ë„ˆë¬´ í¬ë©´ ë¦¬ì‚¬ì´ì¦ˆ)
            if max(img.size) > 1024:
                img.thumbnail((1024, 1024), Image.Resampling.LANCZOS)
            
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = f"USER: <image>\n{question} ASSISTANT:"
            
            # ì…ë ¥ ì¤€ë¹„
            inputs = self.processor(
                text=prompt, 
                images=img, 
                return_tensors="pt"
            ).to(self.model.device)
            
            # ì¶”ë¡  ì‹¤í–‰
            with torch.no_grad():
                output = self.model.generate(
                    **inputs, 
                    max_new_tokens=max_tokens,
                    do_sample=False,  # ì¼ê´€ì„±ì„ ìœ„í•´ greedy decoding
                    pad_token_id=self.processor.tokenizer.eos_token_id,
                    temperature=1.0,  # ëª…ì‹œì  ì„¤ì •
                    use_cache=True    # ì†ë„ í–¥ìƒ
                )
            
            # ì‘ë‹µ ë””ì½”ë”©
            full_response = self.processor.decode(output[0], skip_special_tokens=True)
            
            # "ASSISTANT:" ì´í›„ ë¶€ë¶„ë§Œ ì¶”ì¶œ
            if "ASSISTANT:" in full_response:
                answer = full_response.split("ASSISTANT:")[-1].strip()
            else:
                answer = full_response.strip()
            
            # ë¹ˆ ì‘ë‹µ ì²´í¬
            if not answer:
                answer = "[ëª¨ë¸ì´ ë¹ˆ ì‘ë‹µì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤]"
            
            return {
                'success': True,
                'answer': answer,
                'image_path': image_path,
                'question': question
            }
            
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'image_path': image_path,
                'question': question
            }
    
    def process_multiple_images(self, image_paths, questions):
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ ì²˜ë¦¬"""
        results = []
        
        # questionsê°€ ë¬¸ìì—´ì´ë©´ ëª¨ë“  ì´ë¯¸ì§€ì— ë™ì¼í•œ ì§ˆë¬¸ ì ìš©
        if isinstance(questions, str):
            questions = [questions] * len(image_paths)
        
        # questionsê°€ ë¦¬ìŠ¤íŠ¸ì§€ë§Œ ê¸¸ì´ê°€ 1ì´ë©´ ëª¨ë“  ì´ë¯¸ì§€ì— ì ìš©
        elif len(questions) == 1:
            questions = questions * len(image_paths)
        
        # ê¸¸ì´ê°€ ë§ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬
        elif len(questions) != len(image_paths):
            raise ValueError(f"ì´ë¯¸ì§€ ê°œìˆ˜({len(image_paths)})ì™€ ì§ˆë¬¸ ê°œìˆ˜({len(questions)})ê°€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        print(f"ğŸ¯ ì´ {len(image_paths)}ê°œ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘")
        
        for i, (img_path, question) in enumerate(zip(image_paths, questions)):
            print(f"ğŸ–¼ï¸ [{i+1}/{len(image_paths)}] ì²˜ë¦¬ ì¤‘: {os.path.basename(img_path)}")
            
            result = self.generate_response(img_path, question)
            results.append(result)
            
            if result['success']:
                # ì‘ë‹µ ì¼ë¶€ ë¯¸ë¦¬ë³´ê¸°
                preview = result['answer'][:50] + "..." if len(result['answer']) > 50 else result['answer']
                print(f"âœ… ì™„ë£Œ: {preview}")
            else:
                print(f"âŒ ì‹¤íŒ¨: {result['error']}")
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ (ì„ íƒì‚¬í•­)
            if i % 10 == 0 and i > 0:
                torch.cuda.empty_cache()
        
        return results
    
    def cleanup(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if hasattr(self, 'model'):
            del self.model
        if hasattr(self, 'processor'):
            del self.processor
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        print("ğŸ—‘ï¸ ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")